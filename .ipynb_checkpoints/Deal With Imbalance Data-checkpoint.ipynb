{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qinhui Xu 09/13/2018\n",
    "\n",
    "#### Data Source: https://www.kaggle.com/mlg-ulb/creditcardfraud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the data exploration step shows, the dataset is a real imbalanced one. There is only less than 2% fraud records in the dataset. And actually when we predict wheter the record is a fraud or non-fraud, we care more about true positive rate (if we can predict right about fraud  data). Without dealing with imbalance existing in the dataset, model can easily achieve a good rate of accuracy but cannot achieve a good rate  of sensitivity. Therefore, dealing with imbalance is really essential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Jupyter Notebook, I am going to use six differnet undersampling or oversampling methods to deal with the imbalance existing in the dataset.\n",
    "Before we deal with imbalance, I am going to calculate Sensitivity and Specificity from simple **Logistic Regression** with original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tiffany Xu\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import recall_score,accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing based on Data Exploration / Feature Engineering\n",
    "df = pd.read_csv(\"C:/Users/Tiffany Xu/Documents/MachineLearningStudy/DeepLearning/creditcard.csv\")\n",
    "\n",
    "df['V1_'] = df.V1.map(lambda x: 1 if x < -3 else 0)\n",
    "df['V2_'] = df.V2.map(lambda x: 1 if x > 2.5 else 0)\n",
    "df['V3_'] = df.V3.map(lambda x: 1 if x < -4 else 0)\n",
    "df['V4_'] = df.V4.map(lambda x: 1 if x > 2.5 else 0)\n",
    "df['V5_'] = df.V5.map(lambda x: 1 if x < -4.5 else 0)\n",
    "df['V6_'] = df.V6.map(lambda x: 1 if x < -2.5 else 0)\n",
    "df['V7_'] = df.V7.map(lambda x: 1 if x < -3 else 0)\n",
    "df['V9_'] = df.V9.map(lambda x: 1 if x < -2 else 0)\n",
    "df['V10_'] = df.V10.map(lambda x: 1 if x < -2.5 else 0)\n",
    "df['V11_'] = df.V11.map(lambda x: 1 if x > 2 else 0)\n",
    "df['V12_'] = df.V12.map(lambda x: 1 if x < -2 else 0)\n",
    "df['V14_'] = df.V14.map(lambda x: 1 if x < -2.5 else 0)\n",
    "df['V16_'] = df.V16.map(lambda x: 1 if x < -2 else 0)\n",
    "df['V17_'] = df.V17.map(lambda x: 1 if x < -2 else 0)\n",
    "df['V18_'] = df.V18.map(lambda x: 1 if x < -2 else 0)\n",
    "df['V19_'] = df.V19.map(lambda x: 1 if x > 1.5 else 0)\n",
    "df['V21_'] = df.V21.map(lambda x: 1 if x > 0.6 else 0)\n",
    "\n",
    "df = df.drop(['V28','V27','V26','V25','V24','V23','V22','V20','V15','V13','V8'], axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Logistic Regression without Dealing with Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity/Recall is: 0.6938775510204082\n",
      "Specificity is: 0.9997772462952542\n"
     ]
    }
   ],
   "source": [
    "df['normalized_amount'] = StandardScaler().fit_transform(df['Amount'].values.reshape(-1,1))\n",
    "df = df.drop(['Amount','Time'], axis=1)\n",
    "X = df.loc[:,df.columns != 'Class']\n",
    "y = df.loc[:,df.columns == 'Class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state = 12)\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train,y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "print(\"Sensitivity/Recall is:\",tp/(tp+fn))\n",
    "print(\"Specificity is:\",tn/(tn+fp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result above, we can see that compared to Specificity, *Recall is pretty low*,which is only a little bit better than random guess. Then, I will use different methods to deal with imbalance existing in the dataset and then calculate sensitivity and make a comparsion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_notfraud = df[df[\"Class\"]==0]\n",
    "df_fraud = df[df[\"Class\"]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Our "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
