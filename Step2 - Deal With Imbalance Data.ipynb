{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qinhui Xu 09/15/2018\n",
    "\n",
    "#### Data Source: https://www.kaggle.com/mlg-ulb/creditcardfraud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the data exploration step shows, the dataset is a real imbalanced one. There is only less than 2% fraud records in the dataset. And actually when we predict wheter the record is a fraud or non-fraud, we care more about true positive rate (if we can predict right about fraud  data). Without dealing with imbalance existing in the dataset, model can easily achieve a good rate of accuracy but cannot achieve a good rate  of sensitivity. Therefore, dealing with imbalance is really essential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Jupyter Notebook, I am going to use six differnet undersampling or oversampling methods to deal with the imbalance existing in the dataset.\n",
    "Before we deal with imbalance, I am going to calculate Sensitivity and Specificity from simple **Logistic Regression** with original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tiffany Xu\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import recall_score,accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing based on Data Exploration / Feature Engineering\n",
    "df = pd.read_csv(\"C:/Users/Tiffany Xu/Documents/MachineLearningStudy/DeepLearning/creditcard.csv\")\n",
    "\n",
    "df['V1_'] = df.V1.map(lambda x: 1 if x < -3 else 0)\n",
    "df['V2_'] = df.V2.map(lambda x: 1 if x > 2.5 else 0)\n",
    "df['V3_'] = df.V3.map(lambda x: 1 if x < -4 else 0)\n",
    "df['V4_'] = df.V4.map(lambda x: 1 if x > 2.5 else 0)\n",
    "df['V5_'] = df.V5.map(lambda x: 1 if x < -4.5 else 0)\n",
    "df['V6_'] = df.V6.map(lambda x: 1 if x < -2.5 else 0)\n",
    "df['V7_'] = df.V7.map(lambda x: 1 if x < -3 else 0)\n",
    "df['V9_'] = df.V9.map(lambda x: 1 if x < -2 else 0)\n",
    "df['V10_'] = df.V10.map(lambda x: 1 if x < -2.5 else 0)\n",
    "df['V11_'] = df.V11.map(lambda x: 1 if x > 2 else 0)\n",
    "df['V12_'] = df.V12.map(lambda x: 1 if x < -2 else 0)\n",
    "df['V14_'] = df.V14.map(lambda x: 1 if x < -2.5 else 0)\n",
    "df['V16_'] = df.V16.map(lambda x: 1 if x < -2 else 0)\n",
    "df['V17_'] = df.V17.map(lambda x: 1 if x < -2 else 0)\n",
    "df['V18_'] = df.V18.map(lambda x: 1 if x < -2 else 0)\n",
    "df['V19_'] = df.V19.map(lambda x: 1 if x > 1.5 else 0)\n",
    "df['V21_'] = df.V21.map(lambda x: 1 if x > 0.6 else 0)\n",
    "\n",
    "df = df.drop(['V28','V27','V26','V25','V24','V23','V22','V20','V15','V13','V8'], axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Logistic Regression without Dealing with Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity/Recall is: 0.6938775510204082\n",
      "Specificity is: 0.9997772462952542\n"
     ]
    }
   ],
   "source": [
    "df['normalized_amount'] = StandardScaler().fit_transform(df['Amount'].values.reshape(-1,1))\n",
    "df = df.drop(['Amount','Time'], axis=1)\n",
    "X = df.loc[:,df.columns != 'Class']\n",
    "y = df.loc[:,df.columns == 'Class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state = 12)\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train,y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "print(\"Sensitivity/Recall is:\",tp/(tp+fn))\n",
    "print(\"Specificity is:\",tn/(tn+fp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result above, we can see that compared to Specificity, *Recall is pretty low*,which is only a little bit better than random guess. Then, I will use different methods to deal with imbalance existing in the dataset and then calculate sensitivity and make a comparsion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_notfraud = df[df[\"Class\"]==0]\n",
    "df_fraud = df[df[\"Class\"]==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The methods to deal with imbalance can be mainly divided into three kinds: \n",
    "    \n",
    "   1.Undersampling - reduce the size of majority (non-fraud) to match the minority (fraud)\n",
    "   \n",
    "   2.Oversampling - increase the size of minority (fraud)\n",
    "   \n",
    "   3.Oversampling followed by Undersampling - increase the size first then use undersampling technique to deal with some potential issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity/Recall is: 0.9078947368421053\n",
      "Specificity is: 0.9722222222222222\n"
     ]
    }
   ],
   "source": [
    "df_notfraud_sample = df_notfraud.sample(len(df_fraud))\n",
    "df_random_undersample = pd.concat([df_notfraud_sample,df_fraud],axis=0)\n",
    "\n",
    "X_random_under = df_random_undersample.loc[:,df_random_undersample.columns != 'Class']\n",
    "y_random_under = df_random_undersample.loc[:,df_random_undersample.columns == 'Class']\n",
    "X_under_train, X_under_test, y_under_train, y_under_test = train_test_split(X_random_under,y_random_under,test_size = 0.3, random_state = 12)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_under_train,y_under_train)\n",
    "y_pred = lr.predict(X_under_test)\n",
    "tn, fp, fn, tp = confusion_matrix(y_under_test, y_pred).ravel()\n",
    "print(\"Sensitivity/Recall is:\",tp/(tp+fn))\n",
    "print(\"Specificity is:\",tn/(tn+fp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using random undersampling, Sensitivity is hugely increased to **0.9079**. But random undersampling will potentially cause the loss of information. Therefore, I will try to do undersampling based on K-means clusters. In this way, I can extract a specific number of cluster centroids, which can represent characteristics of each cluster to prevent from information loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster based Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321. connected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>1 hour 48 mins</td></tr>\n",
       "<tr><td>H2O cluster timezone:</td>\n",
       "<td>America/New_York</td></tr>\n",
       "<tr><td>H2O data parsing timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.20.0.2</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>3 months and 3 days </td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>H2O_from_python_Tiffany_Xu_osupl8</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>3.332 Gb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://localhost:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>Algos, AutoML, Core V3, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>3.6.5 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ---------------------------------\n",
       "H2O cluster uptime:         1 hour 48 mins\n",
       "H2O cluster timezone:       America/New_York\n",
       "H2O data parsing timezone:  UTC\n",
       "H2O cluster version:        3.20.0.2\n",
       "H2O cluster version age:    3 months and 3 days\n",
       "H2O cluster name:           H2O_from_python_Tiffany_Xu_osupl8\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    3.332 Gb\n",
       "H2O cluster total cores:    4\n",
       "H2O cluster allowed cores:  4\n",
       "H2O cluster status:         locked, healthy\n",
       "H2O connection url:         http://localhost:54321\n",
       "H2O connection proxy:\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         Algos, AutoML, Core V3, Core V4\n",
       "Python version:             3.6.5 final\n",
       "--------------------------  ---------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import h2o\n",
    "h2o.init()\n",
    "import imp\n",
    "from h2o.estimators.kmeans import H2OKMeansEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "kmeans Model Build progress: |████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "df_kmeans_notfraud = df_notfraud.drop(['Class'], axis=1)\n",
    "hf = h2o.H2OFrame(df_kmeans_notfraud)\n",
    "cls = H2OKMeansEstimator(k=len(df_fraud), standardize=True)\n",
    "cls.train(x=hf.columns, training_frame=hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_centers = pd.DataFrame(cls.centers())\n",
    "df_centers.columns = hf.columns\n",
    "df_centers[\"Class\"] = 0\n",
    "df_kmeans_undersample = pd.concat([df_centers,df_fraud],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity/Recall is: 0.9342105263157895\n",
      "Specificity is: 0.9097222222222222\n"
     ]
    }
   ],
   "source": [
    "X_Kmeans_under = df_kmeans_undersample.loc[:,df_kmeans_undersample.columns != 'Class']\n",
    "y_Kmeans_under = df_kmeans_undersample.loc[:,df_kmeans_undersample.columns == 'Class']\n",
    "X_under_train, X_under_test, y_under_train, y_under_test = train_test_split(X_Kmeans_under,y_Kmeans_under,test_size = 0.3, random_state = 12)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_under_train,y_under_train)\n",
    "y_pred = lr.predict(X_under_test)\n",
    "tn, fp, fn, tp = confusion_matrix(y_under_test, y_pred).ravel()\n",
    "print(\"Sensitivity/Recall is:\",tp/(tp+fn))\n",
    "print(\"Specificity is:\",tn/(tn+fp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that Sensitivity is **increase by 0.027** from the result come from random undersampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMOTE - Synthetic Minority Oversampling Technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tiffany Xu\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:77: DeprecationWarning: Function _ratio_float is deprecated; Use a float for 'ratio' is deprecated from version 0.2. The support will be removed in 0.4. Use a dict, str, or a callable instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity/Recall is: 0.9192237421530965\n",
      "Specificity is: 0.9706824716859339\n"
     ]
    }
   ],
   "source": [
    "df_features = df.drop(['Class'], axis=1)\n",
    "df_target = df[\"Class\"]\n",
    "\n",
    "sm = SMOTE(random_state=12, ratio=1.0)\n",
    "x_res,y_res = sm.fit_sample(df_features,df_target)\n",
    "\n",
    "x_train_res, x_val_res, y_train_res, y_val_res = train_test_split(x_res,\n",
    "                                                    y_res,\n",
    "                                                    test_size = .3,\n",
    "                                                    random_state=12)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(x_train_res,y_train_res)\n",
    "y_pred = lr.predict(x_val_res)\n",
    "tn, fp, fn, tp = confusion_matrix(y_val_res, y_pred).ravel()\n",
    "print(\"Sensitivity/Recall is:\",tp/(tp+fn))\n",
    "print(\"Specificity is:\",tn/(tn+fp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using SMOTE makes sensitivity **0.919**, which is better than random sampling but not as good as undersampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oversampling followed by Undersampling - SMOTE + Tomek Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity/Recall is: 0.9192237421530965\n",
      "Specificity is: 0.9706824716859339\n"
     ]
    }
   ],
   "source": [
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "smote_tomek = SMOTETomek(random_state=12)\n",
    "x_res,y_res = smote_tomek.fit_sample(df_features,df_target)\n",
    "x_train_res, x_val_res, y_train_res, y_val_res = train_test_split(x_res,\n",
    "                                                    y_res,\n",
    "                                                    test_size = .3,\n",
    "                                                    random_state=12)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(x_train_res,y_train_res)\n",
    "y_pred = lr.predict(x_val_res)\n",
    "tn, fp, fn, tp = confusion_matrix(y_val_res, y_pred).ravel()\n",
    "print(\"Sensitivity/Recall is:\",tp/(tp+fn))\n",
    "print(\"Specificity is:\",tn/(tn+fp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The combination of SMOTE and Tomek Links does not improve the result from SMOTE, which indicates that the dataset we created after using SMOTE method does not have significant issues Tomek Links is focused on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oversampling followed by Undersampling - SMOTE + NearMiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tiffany Xu\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:77: DeprecationWarning: Function _ratio_float is deprecated; Use a float for 'ratio' is deprecated from version 0.2. The support will be removed in 0.4. Use a dict, str, or a callable instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity/Recall is: 0.9174981262881768\n",
      "Specificity is: 0.970186743664683\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import NearMiss\n",
    "\n",
    "nr = NearMiss(random_state=12)\n",
    "sm = SMOTE(random_state=12, ratio=1.0)\n",
    "x_res,y_res = sm.fit_sample(df_features,df_target)\n",
    "x_res,y_res = nr.fit_sample(x_res,y_res)\n",
    "\n",
    "x_train_res, x_val_res, y_train_res, y_val_res = train_test_split(x_res,\n",
    "                                                    y_res,\n",
    "                                                    test_size = .3,\n",
    "                                                    random_state=12)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(x_train_res,y_train_res)\n",
    "y_pred = lr.predict(x_val_res)\n",
    "tn, fp, fn, tp = confusion_matrix(y_val_res, y_pred).ravel()\n",
    "print(\"Sensitivity/Recall is:\",tp/(tp+fn))\n",
    "print(\"Specificity is:\",tn/(tn+fp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The combination of SMOTE and NearMiss does change the result, but it makes Sensitivity **lower by 0.02**, which is not what I want. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oversampling followed by Undersampling - SMOTE + ENN (Edited Nearest Neighbor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tiffany Xu\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:77: DeprecationWarning: Function _ratio_float is deprecated; Use a float for 'ratio' is deprecated from version 0.2. The support will be removed in 0.4. Use a dict, str, or a callable instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity/Recall is: 0.918257344066744\n",
      "Specificity is: 0.9708363687636594\n"
     ]
    }
   ],
   "source": [
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "se = SMOTEENN(random_state=12, ratio=1.0)\n",
    "x_res,y_res = se.fit_sample(df_features,df_target)\n",
    "\n",
    "x_train_res, x_val_res, y_train_res, y_val_res = train_test_split(x_res,\n",
    "                                                    y_res,\n",
    "                                                    test_size = .3,\n",
    "                                                    random_state=12)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(x_train_res,y_train_res)\n",
    "y_pred = lr.predict(x_val_res)\n",
    "tn, fp, fn, tp = confusion_matrix(y_val_res, y_pred).ravel()\n",
    "print(\"Sensitivity/Recall is:\",tp/(tp+fn))\n",
    "print(\"Specificity is:\",tn/(tn+fp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The combination of SMOTE and ENN does not improve the result from sensitivity come from only SMOTE method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Though sensitivity has highest value with method of cluster based undersampling,undersampling would still potentially cause loss of information existing in the dataset.\n",
    "\n",
    "#### Therefore, I will use try building models based on the data come from cluster base undersampling and SMOTE seperately,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
